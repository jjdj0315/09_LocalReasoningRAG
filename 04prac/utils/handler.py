# utils/handler.py

import streamlit as st
import json
from langchain_core.messages import ChatMessage
from langchain_core.documents import Document  # Document íƒ€ì… ì„í¬íŠ¸
from utils.dataclass import ChatMessageWithType  # dataclass.pyì—ì„œ ì •ì˜ëœ í´ë˜ìŠ¤


def format_search_result(results):
    """
    RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    """
    if isinstance(results, list) and all(isinstance(r, Document) for r in results):
        answer = ""
        for i, doc in enumerate(results):
            answer += f'**[{i+1}] {doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")}**\n'
            answer += (
                f"```\n{doc.page_content[:300]}...\n```\n\n"  # ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ë‚´ìš© í‘œì‹œ
            )
        return answer
    else:
        return str(results)  # ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…ì˜ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜


def stream_handler(streamlit_container, langgraph_app_stream_generator):
    """
    LangGraph ì•±ì˜ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ê³  Streamlit UIì— ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    ê° ë…¸ë“œì˜ ì‹¤í–‰ ìƒíƒœë¥¼ í‘œì‹œí•˜ê³ , ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ìµœì¢… ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

    Args:
        streamlit_container (streamlit.empty): ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  Streamlit ì»¨í…Œì´ë„ˆ.
        langgraph_app_stream_generator (iterator): app.stream()ì´ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°.

    Returns:
        tuple: (list_of_tool_results, final_answer_string, final_thinking_string)
        tool_resultsëŠ” LangGraphì˜ "documents" ë…¸ë“œì—ì„œ ìƒì„±ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    current_agent_answer_buffer = ""  # LLM ë‹µë³€ ë²„í¼ (ìŠ¤íŠ¸ë¦¬ë° ìš©)
    retrieved_documents = []  # ê²€ìƒ‰ëœ ë¬¸ì„œ (ìµœì¢… session_state ì €ì¥ì„ ìœ„í•¨)
    thinking_process = ""  # ì¶”ë¡  ê³¼ì • (ìµœì¢… session_state ì €ì¥ì„ ìœ„í•¨)

    # ìŠ¤íŠ¸ë¦¬ë°ëœ LLM ë‹µë³€ì„ í‘œì‹œí•  Streamlit.empty() ê°ì²´.
    # ì´ ê°ì²´ëŠ” stream_handler ì™¸ë¶€ì—ì„œ app.pyê°€ ìƒì„±í•˜ì—¬ ë„˜ê²¨ì£¼ë¯€ë¡œ, ì´ í•¨ìˆ˜ ë‚´ì—ì„œ ë‹¤ì‹œ ìƒì„±í•  í•„ìš” ì—†ìŒ.
    # if not hasattr(streamlit_container, 'markdown'):
    #     st.error("ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ì— ìœ íš¨í•œ Streamlit ì»¨í…Œì´ë„ˆê°€ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    #     return [], "", "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ê°’ ë°˜í™˜

    # ê° ë…¸ë“œë³„ ìƒíƒœ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ì„ì‹œ ì»¨í…Œì´ë„ˆ
    node_status_container = st.empty()

    # ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
    for chunk in langgraph_app_stream_generator:
        # chunkëŠ” { "node_name": state_delta } í˜•íƒœ
        for node_name, node_output in chunk.items():
            # print(f"Chunk from node '{node_name}': {node_output}") # ë””ë²„ê¹…ìš©

            # --- ë…¸ë“œ ìƒíƒœ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ ---
            if node_name == "classify":
                node_status_container.info("ğŸ” ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘...")
            elif node_name == "retrieve":
                node_status_container.info("ğŸ“š ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
                if "documents" in node_output and node_output["documents"]:
                    retrieved_documents = node_output["documents"]  # ë¬¸ì„œ ì €ì¥
                    # ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë³„ë„ì˜ expanderì— í‘œì‹œ
                    with streamlit_container:  # ì£¼ ìŠ¤íŠ¸ë¦¬ë° ì»¨í…Œì´ë„ˆ ë‚´ì— expander ìƒì„±
                        with st.expander("âœ… ê²€ìƒ‰ëœ ë¬¸ì„œ"):
                            st.markdown(format_search_result(retrieved_documents))
                    node_status_container.empty()  # ê²€ìƒ‰ ì™„ë£Œ í›„ ìƒíƒœ ë©”ì‹œì§€ ì œê±°
            elif node_name == "reasoning":
                node_status_container.info("ğŸ¤” ì¶”ë¡  ê³¼ì • ìƒì„± ì¤‘...")
                if "thinking" in node_output and node_output["thinking"]:
                    thinking_process = node_output["thinking"]  # ì¶”ë¡  ê³¼ì • ì €ì¥
                    # ì¶”ë¡  ê³¼ì •ë„ expanderì— í‘œì‹œ
                    with streamlit_container:
                        with st.expander("ğŸ§  ì¶”ë¡  ê³¼ì •"):
                            st.markdown(thinking_process)
                    node_status_container.empty()  # ì¶”ë¡  ì™„ë£Œ í›„ ìƒíƒœ ë©”ì‹œì§€ ì œê±°
            elif node_name == "generate":
                node_status_container.info("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...")
                # LLM ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°ì€ 'answer' í•„ë“œì— ëˆ„ì ëœ ë‚´ìš©ì´ ë“¤ì–´ì˜µë‹ˆë‹¤.
                if "answer" in node_output:
                    current_agent_answer_buffer = node_output["answer"]
                    streamlit_container.markdown(
                        current_agent_answer_buffer
                    )  # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
            elif node_name == "__end__":
                node_status_container.empty()  # ëª¨ë“  ì‘ì—… ì™„ë£Œ í›„ ìƒíƒœ ë©”ì‹œì§€ ì œê±°

    # ìŠ¤íŠ¸ë¦¬ë°ì´ ëª¨ë‘ ëë‚œ í›„, ìµœì¢… ê²°ê³¼ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    return retrieved_documents, current_agent_answer_buffer, thinking_process
